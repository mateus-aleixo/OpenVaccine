{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:25.407171Z",
     "iopub.status.busy": "2025-06-19T11:48:25.406680Z",
     "iopub.status.idle": "2025-06-19T11:48:39.397776Z",
     "shell.execute_reply": "2025-06-19T11:48:39.396953Z",
     "shell.execute_reply.started": "2025-06-19T11:48:25.407147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:39.399546Z",
     "iopub.status.busy": "2025-06-19T11:48:39.399046Z",
     "iopub.status.idle": "2025-06-19T11:48:39.404724Z",
     "shell.execute_reply": "2025-06-19T11:48:39.403942Z",
     "shell.execute_reply.started": "2025-06-19T11:48:39.399515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mcrmse(t, p, seq_len_target):\n",
    "    t = t[:, :seq_len_target]\n",
    "    p = p[:, :seq_len_target]\n",
    "    score = np.mean(np.sqrt(np.mean(np.mean((p - t) ** 2, axis=1), axis=0)))\n",
    "    return score\n",
    "\n",
    "\n",
    "def mcrmse_loss(t, y, seq_len_target):\n",
    "    t = t[:, :seq_len_target]\n",
    "    y = y[:, :seq_len_target]\n",
    "    loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean(tf.reduce_mean((t - y) ** 2, axis=1), axis=0)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Layer for the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:39.405789Z",
     "iopub.status.busy": "2025-06-19T11:48:39.405550Z",
     "iopub.status.idle": "2025-06-19T11:48:39.439211Z",
     "shell.execute_reply": "2025-06-19T11:48:39.438530Z",
     "shell.execute_reply.started": "2025-06-19T11:48:39.405772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AutoencoderLossLayer(L.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AutoencoderLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        node, p = inputs\n",
    "        # Custom loss for denoising autoencoder (binary cross-entropy like)\n",
    "        loss = -tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention and Model Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:39.440901Z",
     "iopub.status.busy": "2025-06-19T11:48:39.440690Z",
     "iopub.status.idle": "2025-06-19T11:48:39.460521Z",
     "shell.execute_reply": "2025-06-19T11:48:39.459824Z",
     "shell.execute_reply.started": "2025-06-19T11:48:39.440885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def attention(x_inner, x_outer, n_factor):\n",
    "    x_Q = L.Conv1D(n_factor, 1, activation='linear',\n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   bias_initializer='glorot_uniform')(x_inner)\n",
    "    x_K = L.Conv1D(n_factor, 1, activation='linear',\n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   bias_initializer='glorot_uniform')(x_outer)\n",
    "    x_V = L.Conv1D(n_factor, 1, activation='linear',\n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   bias_initializer='glorot_uniform')(x_outer)\n",
    "    x_KT = L.Permute((2, 1))(x_K)\n",
    "    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n",
    "    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n",
    "    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n",
    "    return att\n",
    "\n",
    "\n",
    "def multi_head_attention(x, y, n_factor, n_head, dropout):\n",
    "    if n_head == 1:\n",
    "        att = attention(x, y, n_factor)\n",
    "    else:\n",
    "        n_factor_head = n_factor // n_head\n",
    "        heads = [attention(x, y, n_factor_head) for i in range(n_head)]\n",
    "        att = L.Concatenate()(heads)\n",
    "        att = L.Dense(n_factor,\n",
    "                      kernel_initializer='glorot_uniform',\n",
    "                      bias_initializer='glorot_uniform')(att)\n",
    "    x = L.Add()([x, att])\n",
    "    x = L.LayerNormalization()(x)\n",
    "    if dropout > 0:\n",
    "        x = L.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def res(x, unit, kernel=3, rate=0.1):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding=\"same\", activation=None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    return L.Add()([x, h])\n",
    "\n",
    "\n",
    "def forward(x, unit, kernel=3, rate=0.1):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding=\"same\", activation=None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = res(h, unit, kernel, rate)\n",
    "    return h\n",
    "\n",
    "\n",
    "def adj_attn(x, adj, unit, n=2, rate=0.1):\n",
    "    x_a = x\n",
    "    x_as = []\n",
    "    for i in range(n):\n",
    "        x_a = forward(x_a, unit)\n",
    "        x_a = L.Lambda(lambda inputs: tf.matmul(inputs[0], inputs[1]))([adj, x_a])\n",
    "        x_as.append(x_a)\n",
    "    if n == 1:\n",
    "        x_a = x_as[0]\n",
    "    else:\n",
    "        x_a = L.Concatenate()(x_as)\n",
    "    x_a = forward(x_a, unit)\n",
    "    return x_a\n",
    "\n",
    "\n",
    "def get_base(X_node_shape_2, As_shape_3):\n",
    "    node = tf.keras.Input(shape=(None, X_node_shape_2), name=\"node\")\n",
    "    adj = tf.keras.Input(shape=(None, None, As_shape_3), name=\"adj\")\n",
    "\n",
    "    # Learnable adjacency component\n",
    "    adj_learned = L.Dense(1, \"relu\")(adj)\n",
    "    adj_all = L.Concatenate(axis=3)([adj, adj_learned])\n",
    "\n",
    "    # Initial feature extraction using multiple Conv1D layers with different kernel sizes\n",
    "    xs = []\n",
    "    xs.append(node)\n",
    "    x1 = forward(node, 128, kernel=3, rate=0.0)\n",
    "    x2 = forward(x1, 64, kernel=6, rate=0.0)\n",
    "    x3 = forward(x2, 32, kernel=15, rate=0.0)\n",
    "    x4 = forward(x3, 16, kernel=30, rate=0.0)\n",
    "    x = L.Concatenate()([x1, x2, x3, x4])\n",
    "\n",
    "    # Graph convolution and multi-head attention layers\n",
    "    for unit in [64, 32]:\n",
    "        x_as = []\n",
    "        for i in range(adj_all.shape[3]):\n",
    "            adj_slice = L.Lambda(lambda inputs, idx=i: inputs[:, :, :, idx])(adj_all)\n",
    "            x_a = adj_attn(x, adj_slice, unit, rate=0.0)\n",
    "            x_as.append(x_a)\n",
    "        x_c = forward(x, unit, kernel=30)\n",
    "        \n",
    "        x = L.Concatenate()(x_as + [x_c])\n",
    "        x = forward(x, unit)\n",
    "        x = multi_head_attention(x, x, unit, 4, 0.0)\n",
    "        xs.append(x)\n",
    "        \n",
    "    x = L.Concatenate()(xs)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node, adj], outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ae_model(base_model, X_node_shape_2):\n",
    "    node = tf.keras.Input(shape=(None, X_node_shape_2), name=\"node\")\n",
    "    adj = tf.keras.Input(shape=(None, None, base_model.input[1].shape[3]), name=\"adj\")\n",
    "\n",
    "    # Apply spatial dropout to the node input for denoising\n",
    "    x = base_model([L.SpatialDropout1D(0.3)(node), adj])    \n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "    p = L.Dense(X_node_shape_2, activation='sigmoid')(x)\n",
    "\n",
    "    loss = AutoencoderLossLayer()([node, p])\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node, adj], outputs=[loss])\n",
    "    \n",
    "    opt = tf.optimizers.Adam()\n",
    "    model.compile(optimizer=opt, loss=lambda t, y: y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(base_model, seq_len_target, As_shape_3):\n",
    "    node = tf.keras.Input(shape=(None, base_model.input[0].shape[2]), name=\"node\")\n",
    "    adj = tf.keras.Input(shape=(None, None, As_shape_3), name=\"adj\")\n",
    "    \n",
    "    x = base_model([node, adj])\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    x = L.Dropout(0.4)(x)\n",
    "    x = L.Dense(5, activation=None)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node, adj], outputs=[x])\n",
    "    \n",
    "    opt = tf.optimizers.Adam()\n",
    "    # Using the custom MCRMSE loss for compilation\n",
    "    model.compile(optimizer=opt, loss=lambda t, y: mcrmse_loss(t, y, seq_len_target))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:39.461536Z",
     "iopub.status.busy": "2025-06-19T11:48:39.461283Z",
     "iopub.status.idle": "2025-06-19T11:49:19.950143Z",
     "shell.execute_reply": "2025-06-19T11:49:19.949542Z",
     "shell.execute_reply.started": "2025-06-19T11:48:39.461519Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b26352612b482aa69045b8ffbf1251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading BPPs:   0%|          | 0/2096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4473dd34d8cc4186b01ca90874bdb767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading BPPs:   0%|          | 0/629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484b5ed654a0442cbe04923ce8e88ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading BPPs:   0%|          | 0/3005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "train = pd.read_json(\"./stanford-covid-vaccine/train.json\", lines=True)\n",
    "test  = pd.read_json(\"./stanford-covid-vaccine/test.json\", lines=True)\n",
    "sub = pd.read_csv(\"./stanford-covid-vaccine/sample_submission.csv\")\n",
    "\n",
    "# # Filter training data\n",
    "train = train[train.signal_to_noise > 1].reset_index(drop=True)\n",
    "\n",
    "# Separate public and private test data based on sequence length\n",
    "test_pub = test[test[\"seq_length\"] == 107].reset_index(drop=True)\n",
    "test_pri = test[test[\"seq_length\"] == 130].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Load BPP (Base Pair Probability) matrices\n",
    "def load_bpps(df, path=\"./stanford-covid-vaccine/bpps/\"):\n",
    "    \"\"\"Loads BPP matrices for a given DataFrame.\"\"\"\n",
    "    bpps = []\n",
    "    for id_val in tqdm(df[\"id\"], desc=\"Loading BPPs\"):\n",
    "        bpps.append(np.load(f\"{path}{id_val}.npy\"))\n",
    "    return np.array(bpps)\n",
    "\n",
    "\n",
    "As = load_bpps(train)\n",
    "As_pub = load_bpps(test_pub)\n",
    "As_pri = load_bpps(test_pri)\n",
    "\n",
    "# Get target column names from sample submission\n",
    "targets = list(sub.columns[1:])\n",
    "\n",
    "# Prepare y_train (target values for training)\n",
    "seq_len_train_example = train[\"seq_length\"].iloc[0]\n",
    "seq_len_target_example = train[\"seq_scored\"].iloc[0]\n",
    "ignore_value = -10000\n",
    "ignore_length = seq_len_train_example - seq_len_target_example\n",
    "\n",
    "y_train = []\n",
    "for target in targets:\n",
    "    y = np.vstack(train[target])\n",
    "    # Pad with ignore_value for positions not scored\n",
    "    dummy = np.zeros([y.shape[0], ignore_length]) + ignore_value\n",
    "    y = np.hstack([y, dummy])\n",
    "    y_train.append(y)\n",
    "y = np.stack(y_train, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:49:19.951136Z",
     "iopub.status.busy": "2025-06-19T11:49:19.950891Z",
     "iopub.status.idle": "2025-06-19T11:49:28.724989Z",
     "shell.execute_reply": "2025-06-19T11:49:28.724214Z",
     "shell.execute_reply.started": "2025-06-19T11:49:19.951118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b170dc9fd047f7821c21e758ddb33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting Structure Adjacency:   0%|          | 0/2096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4066712c73ef47c9a0aa475d96ad443d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting Structure Adjacency:   0%|          | 0/629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85bfeaadcb54d46a52cba68165c5a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting Structure Adjacency:   0%|          | 0/3005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_structure_adj(df):\n",
    "    Ss = []\n",
    "    for i in tqdm(range(len(df)), desc=\"Getting Structure Adjacency\"):\n",
    "        seq_length = df[\"seq_length\"].iloc[i]\n",
    "        structure = df[\"structure\"].iloc[i]\n",
    "        sequence = df[\"sequence\"].iloc[i]\n",
    "\n",
    "        cue = []\n",
    "        # Separate adjacency matrices for different base pairs, then sum them up\n",
    "        a_structures = {\n",
    "            (\"A\", \"U\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"C\", \"G\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"G\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"A\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"C\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"U\"): np.zeros([seq_length, seq_length]),\n",
    "        }\n",
    "        \n",
    "        for j in range(seq_length):\n",
    "            if structure[j] == \"(\":\n",
    "                cue.append(j)\n",
    "            elif structure[j] == \")\":\n",
    "                if cue:\n",
    "                    start = cue.pop()\n",
    "                    a_structures[(sequence[start], sequence[j])][start, j] = 1\n",
    "                    a_structures[(sequence[j], sequence[start])][j, start] = 1\n",
    "        \n",
    "        # Sum all specific base pair adjacency matrices into a single one\n",
    "        a_strc = np.stack([a for a in a_structures.values()], axis=2)\n",
    "        a_strc = np.sum(a_strc, axis=2, keepdims=True)\n",
    "        Ss.append(a_strc)\n",
    "    \n",
    "    return np.array(Ss)\n",
    "\n",
    "\n",
    "Ss = get_structure_adj(train)\n",
    "Ss_pub = get_structure_adj(test_pub)\n",
    "Ss_pri = get_structure_adj(test_pri)\n",
    "\n",
    "\n",
    "def get_distance_matrix(bpps_array):\n",
    "    seq_length = bpps_array.shape[1]\n",
    "    idx = np.arange(seq_length)\n",
    "    Ds = []\n",
    "    for i in range(len(idx)):\n",
    "        d = np.abs(idx[i] - idx)\n",
    "        Ds.append(d)\n",
    "\n",
    "    Ds = np.array(Ds) + 1\n",
    "    Ds = 1 / Ds\n",
    "    Ds = Ds[None, :, :]\n",
    "    Ds = np.repeat(Ds, len(bpps_array), axis=0)\n",
    "    \n",
    "    Dss = []\n",
    "    for i in [1, 2, 4]:\n",
    "        Dss.append(Ds ** i)\n",
    "    Ds = np.stack(Dss, axis=3)\n",
    "    return Ds\n",
    "\n",
    "Ds = get_distance_matrix(As)\n",
    "Ds_pub = get_distance_matrix(As_pub)\n",
    "Ds_pri = get_distance_matrix(As_pri)\n",
    "\n",
    "# Concatenate all adjacency features: BPPs, Structure Adjacency, Distance Adjacency\n",
    "As = np.concatenate([As[:, :, :, None], Ss, Ds], axis=3).astype(np.float32)\n",
    "As_pub = np.concatenate([As_pub[:, :, :, None], Ss_pub, Ds_pub], axis=3).astype(np.float32)\n",
    "As_pri = np.concatenate([As_pri[:, :, :, None], Ss_pri, Ds_pri], axis=3).astype(np.float32)\n",
    "\n",
    "# Free up memory\n",
    "del Ss, Ds, Ss_pub, Ds_pub, Ss_pri, Ds_pri\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:49:28.726159Z",
     "iopub.status.busy": "2025-06-19T11:49:28.725968Z",
     "iopub.status.idle": "2025-06-19T11:49:29.874157Z",
     "shell.execute_reply": "2025-06-19T11:49:29.873603Z",
     "shell.execute_reply.started": "2025-06-19T11:49:28.726144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def return_ohe(n, i):\n",
    "    tmp = [0] * n\n",
    "    tmp[i] = 1\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def get_input(df):\n",
    "    # One-hot encode sequence (bases)\n",
    "    mapping_node = {s: return_ohe(4, i) for i, s in enumerate([\"A\", \"G\", \"C\", \"U\"])}\n",
    "    X_node = np.stack(df[\"sequence\"].apply(lambda x: list(map(lambda y: mapping_node[y], list(x)))))\n",
    "\n",
    "    # One-hot encode predicted loop type\n",
    "    mapping_loop = {s: return_ohe(7, i) for i, s in enumerate([\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"])}\n",
    "    X_loop = np.stack(df[\"predicted_loop_type\"].apply(lambda x: list(map(lambda y: mapping_loop[y], list(x)))))\n",
    "    \n",
    "    X_node = np.concatenate([X_node, X_loop], axis=2)\n",
    "    \n",
    "    # Interaction features: create a unique integer for each combination of features\n",
    "    # and then one-hot encode these combined features.\n",
    "    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis=2)\n",
    "    vocab = sorted(set(a.flatten()))\n",
    "    ohes = []\n",
    "    for v in vocab:\n",
    "        ohes.append(a == v)\n",
    "    ohes = np.stack(ohes, axis=2)\n",
    "    \n",
    "    X_node = np.concatenate([X_node, ohes], axis=2).astype(np.float32)\n",
    "    \n",
    "    return X_node\n",
    "\n",
    "\n",
    "X_node = get_input(train)\n",
    "X_node_pub = get_input(test_pub)\n",
    "X_node_pri = get_input(test_pri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training the Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:49:29.875104Z",
     "iopub.status.busy": "2025-06-19T11:49:29.874843Z",
     "iopub.status.idle": "2025-06-19T12:02:33.576146Z",
     "shell.execute_reply": "2025-06-19T12:02:33.575579Z",
     "shell.execute_reply.started": "2025-06-19T11:49:29.875078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training Iteration 1/4\n",
      "Training on main dataset\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 36s 142ms/step - loss: 0.8666\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.2422\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.1132\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.0717\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.0541\n",
      "Training on public test dataset\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 0.0459\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 2s 113ms/step - loss: 0.0436\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 2s 106ms/step - loss: 0.0403\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 2s 102ms/step - loss: 0.0353\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 2s 100ms/step - loss: 0.0320\n",
      "Training on private test dataset\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 26s 127ms/step - loss: 0.0362\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 10s 111ms/step - loss: 0.0315\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 11s 116ms/step - loss: 0.0263\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 10s 109ms/step - loss: 0.0229\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 10s 111ms/step - loss: 0.0182\n",
      "Pre-training Iteration 2/4\n",
      "Training on main dataset\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.0156\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.0131\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.0145\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.0126\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.0117\n",
      "Training on public test dataset\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 2s 104ms/step - loss: 0.0129\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 2s 100ms/step - loss: 0.0129\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 2s 105ms/step - loss: 0.0119\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 2s 109ms/step - loss: 0.0121\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 2s 101ms/step - loss: 0.0157\n",
      "Training on private test dataset\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 11s 115ms/step - loss: 0.0155\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 11s 114ms/step - loss: 0.0137\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 11s 118ms/step - loss: 0.0108\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 10s 110ms/step - loss: 0.0135\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 10s 109ms/step - loss: 0.0126\n",
      "Pre-training Iteration 3/4\n",
      "Training on main dataset\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.0118\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.0091\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.0083\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.0078\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.0074\n",
      "Training on public test dataset\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 2s 117ms/step - loss: 0.0080\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 2s 102ms/step - loss: 0.0064\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 2s 101ms/step - loss: 0.0081\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 2s 105ms/step - loss: 0.0063\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 2s 100ms/step - loss: 0.0051\n",
      "Training on private test dataset\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 11s 116ms/step - loss: 0.0114\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 11s 116ms/step - loss: 0.0110\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 11s 112ms/step - loss: 0.0086\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 11s 118ms/step - loss: 0.0073\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 10s 107ms/step - loss: 0.0086\n",
      "Pre-training Iteration 4/4\n",
      "Training on main dataset\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.0072\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.0060\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 7s 112ms/step - loss: 0.0067\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.0079\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.0076\n",
      "Training on public test dataset\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 2s 103ms/step - loss: 0.0081\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 2s 102ms/step - loss: 0.0058\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 2s 99ms/step - loss: 0.0069\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 2s 105ms/step - loss: 0.0049\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 2s 101ms/step - loss: 0.0043\n",
      "Training on private test dataset\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 10s 110ms/step - loss: 0.0073\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 10s 109ms/step - loss: 0.0066\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 10s 107ms/step - loss: 0.0080\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 10s 108ms/step - loss: 0.0067\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 10s 110ms/step - loss: 0.0062\n"
     ]
    }
   ],
   "source": [
    "# Dynamically get input shapes for model construction\n",
    "base_model_ae = get_base(X_node.shape[2], As.shape[3])\n",
    "ae_model = get_ae_model(base_model_ae, X_node.shape[2])\n",
    "\n",
    "# Train denoising autoencoder using all data (train, public test, private test)\n",
    "dummy_ae_target = np.zeros_like(X_node[:, :, 0])\n",
    "dummy_ae_target_pub = np.zeros_like(X_node_pub[:, :, 0])\n",
    "dummy_ae_target_pri = np.zeros_like(X_node_pri[:, :, 0])\n",
    "\n",
    "for i in range(20 // 5):\n",
    "    print(f\"Pre-training Iteration {i+1}/{20 // 5}\")\n",
    "    print(\"Training on main dataset\")\n",
    "    ae_model.fit([X_node, As], [dummy_ae_target],\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "    print(\"Training on public test dataset\")\n",
    "    ae_model.fit([X_node_pub, As_pub], [dummy_ae_target_pub],\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "    print(\"Training on private test dataset\")\n",
    "    ae_model.fit([X_node_pri, As_pri], [dummy_ae_target_pri],\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "    gc.collect()\n",
    "base_model_ae.save_weights(\"./base_ae.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training (Regression) with KFold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T12:03:27.529365Z",
     "iopub.status.busy": "2025-06-19T12:03:27.529091Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Start\n",
      "Epochs: 30, Batch size: 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 44s 99ms/step - loss: 0.4362\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 18s 87ms/step - loss: 0.3447\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 22s 107ms/step - loss: 0.3267 - val_loss: 0.3022\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.3125\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 19s 88ms/step - loss: 0.3014\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 20s 98ms/step - loss: 0.2934 - val_loss: 0.2739\n",
      "Epoch 7/30\n",
      "210/210 [==============================] - 18s 87ms/step - loss: 0.2853\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2817\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 21s 100ms/step - loss: 0.2757 - val_loss: 0.2613\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2734\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2702\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 21s 101ms/step - loss: 0.2634 - val_loss: 0.2561\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2619\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2603\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 21s 100ms/step - loss: 0.2572 - val_loss: 0.2480\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 19s 93ms/step - loss: 0.2543\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 20s 94ms/step - loss: 0.2523\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 20s 97ms/step - loss: 0.2504 - val_loss: 0.2466\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2466\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2458\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 20s 97ms/step - loss: 0.2441 - val_loss: 0.2457\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 18s 86ms/step - loss: 0.2421\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2389\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 21s 101ms/step - loss: 0.2370 - val_loss: 0.2402\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2351\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2351\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 20s 97ms/step - loss: 0.2336 - val_loss: 0.2403\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2305\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 18s 88ms/step - loss: 0.2302\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2289 - val_loss: 0.2398\n",
      "Epochs: 10, Batch size: 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 13s 108ms/step - loss: 0.2243\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 10s 97ms/step - loss: 0.2204\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 11s 102ms/step - loss: 0.2193 - val_loss: 0.2344\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 10s 94ms/step - loss: 0.2190\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 10s 94ms/step - loss: 0.2177\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 10s 100ms/step - loss: 0.2177 - val_loss: 0.2347\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 10s 91ms/step - loss: 0.2158\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 10s 96ms/step - loss: 0.2160\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 10s 96ms/step - loss: 0.2150 - val_loss: 0.2349\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 10s 92ms/step - loss: 0.2140\n",
      "Epochs: 3, Batch size: 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 7s 105ms/step - loss: 0.2107\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 5s 99ms/step - loss: 0.2090\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 6s 104ms/step - loss: 0.2082 - val_loss: 0.2337\n",
      "Epochs: 3, Batch size: 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 5s 120ms/step - loss: 0.2071\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 3s 122ms/step - loss: 0.2057\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 0.2054 - val_loss: 0.2320\n",
      "Epochs: 5, Batch size: 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 29s 2s/step - loss: 0.2050\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 23s 2s/step - loss: 0.2046\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.2041 - val_loss: 0.2324\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.2042\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.2027\n",
      "Epochs: 5, Batch size: 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 23s 3s/step - loss: 0.2031\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 22s 3s/step - loss: 0.2024\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 23s 3s/step - loss: 0.2024 - val_loss: 0.2324\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.2019\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.2007\n",
      "Fold 0: MCRMSE 0.2323019532039205\n",
      "Fold 1 Start\n",
      "Epochs: 30, Batch size: 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 35s 94ms/step - loss: 0.4271\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.3451\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 25s 118ms/step - loss: 0.3245 - val_loss: 0.2990\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 20s 93ms/step - loss: 0.3089\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2965\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 20s 96ms/step - loss: 0.2903 - val_loss: 0.2638\n",
      "Epoch 7/30\n",
      "210/210 [==============================] - 19s 88ms/step - loss: 0.2864\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2788\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 20s 96ms/step - loss: 0.2761 - val_loss: 0.2557\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2710\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 20s 93ms/step - loss: 0.2681\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 21s 102ms/step - loss: 0.2651 - val_loss: 0.2576\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 21s 98ms/step - loss: 0.2640\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2581\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 21s 98ms/step - loss: 0.2569 - val_loss: 0.2476\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2537\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 18s 87ms/step - loss: 0.2515\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 21s 99ms/step - loss: 0.2490 - val_loss: 0.2412\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2485\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2469\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 21s 99ms/step - loss: 0.2435 - val_loss: 0.2379\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 19s 93ms/step - loss: 0.2419\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2412\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 21s 99ms/step - loss: 0.2395 - val_loss: 0.2362\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2369\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2356\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 21s 100ms/step - loss: 0.2329 - val_loss: 0.2356\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2331\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 19s 93ms/step - loss: 0.2315\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 21s 98ms/step - loss: 0.2289 - val_loss: 0.2302\n",
      "Epochs: 10, Batch size: 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 11s 105ms/step - loss: 0.2250\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 9s 88ms/step - loss: 0.2222\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 10s 96ms/step - loss: 0.2208 - val_loss: 0.2278\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 9s 90ms/step - loss: 0.2200\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 9s 90ms/step - loss: 0.2181\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 10s 98ms/step - loss: 0.2186 - val_loss: 0.2270\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 9s 90ms/step - loss: 0.2177\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 10s 91ms/step - loss: 0.2167\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 10s 98ms/step - loss: 0.2149 - val_loss: 0.2284\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 10s 92ms/step - loss: 0.2150\n",
      "Epochs: 3, Batch size: 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 5s 98ms/step - loss: 0.2121\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 5s 96ms/step - loss: 0.2100\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 6s 106ms/step - loss: 0.2087 - val_loss: 0.2258\n",
      "Epochs: 3, Batch size: 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 3s 121ms/step - loss: 0.2073\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 3s 122ms/step - loss: 0.2073\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 0.2055 - val_loss: 0.2258\n",
      "Epochs: 5, Batch size: 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 32s 2s/step - loss: 0.2056\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.2052\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 27s 2s/step - loss: 0.2046 - val_loss: 0.2247\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.2048\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 24s 2s/step - loss: 0.2036\n",
      "Epochs: 5, Batch size: 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 24s 3s/step - loss: 0.2041\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 24s 3s/step - loss: 0.2040\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 24s 3s/step - loss: 0.2035 - val_loss: 0.2249\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 23s 3s/step - loss: 0.2033\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 23s 3s/step - loss: 0.2026\n",
      "Fold 1: MCRMSE 0.22499282996635284\n",
      "Fold 2 Start\n",
      "Epochs: 30, Batch size: 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 35s 90ms/step - loss: 0.4358\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.3461\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 23s 112ms/step - loss: 0.3269 - val_loss: 0.3112\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.3129\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2974\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 20s 96ms/step - loss: 0.2895 - val_loss: 0.2832\n",
      "Epoch 7/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2845\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2782\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 21s 100ms/step - loss: 0.2741 - val_loss: 0.2710\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2720\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2662\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 21s 98ms/step - loss: 0.2625 - val_loss: 0.2589\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2588\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2566\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 20s 97ms/step - loss: 0.2557 - val_loss: 0.2534\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2496\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2483\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 20s 97ms/step - loss: 0.2454 - val_loss: 0.2539\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2443\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2425\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 21s 101ms/step - loss: 0.2399 - val_loss: 0.2500\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2392\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2363\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 21s 99ms/step - loss: 0.2364 - val_loss: 0.2466\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2326\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2324\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 20s 96ms/step - loss: 0.2307 - val_loss: 0.2445\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2291\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2281\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 20s 96ms/step - loss: 0.2271 - val_loss: 0.2444\n",
      "Epochs: 10, Batch size: 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 10s 93ms/step - loss: 0.2216\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 10s 97ms/step - loss: 0.2186\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 11s 100ms/step - loss: 0.2179 - val_loss: 0.2399\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 9s 87ms/step - loss: 0.2175\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 9s 88ms/step - loss: 0.2165\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 10s 99ms/step - loss: 0.2157 - val_loss: 0.2397\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 10s 91ms/step - loss: 0.2148\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 9s 89ms/step - loss: 0.2142\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 10s 100ms/step - loss: 0.2143 - val_loss: 0.2385\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 9s 88ms/step - loss: 0.2131\n",
      "Epochs: 3, Batch size: 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 5s 97ms/step - loss: 0.2092\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 5s 95ms/step - loss: 0.2070\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 6s 104ms/step - loss: 0.2067 - val_loss: 0.2396\n",
      "Epochs: 3, Batch size: 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 3s 121ms/step - loss: 0.2061\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 3s 119ms/step - loss: 0.2039\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 0.2027 - val_loss: 0.2373\n",
      "Epochs: 5, Batch size: 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.2033\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.2031\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.2025 - val_loss: 0.2387\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 38s 3s/step - loss: 0.2019\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 38s 3s/step - loss: 0.2016\n",
      "Epochs: 5, Batch size: 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 35s 5s/step - loss: 0.2014\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 33s 5s/step - loss: 0.2009\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 35s 5s/step - loss: 0.2003 - val_loss: 0.2375\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 33s 5s/step - loss: 0.2009\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 33s 5s/step - loss: 0.2003\n",
      "Fold 2: MCRMSE 0.23751222515105547\n",
      "Fold 3 Start\n",
      "Epochs: 30, Batch size: 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 34s 89ms/step - loss: 0.4514\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.3487\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 23s 111ms/step - loss: 0.3247 - val_loss: 0.2924\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 19s 88ms/step - loss: 0.3091\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2955\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 20s 97ms/step - loss: 0.2883 - val_loss: 0.2728\n",
      "Epoch 7/30\n",
      "210/210 [==============================] - 19s 88ms/step - loss: 0.2819\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2766\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 20s 93ms/step - loss: 0.2732 - val_loss: 0.2641\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2680\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 18s 88ms/step - loss: 0.2646\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2617 - val_loss: 0.2577\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 19s 92ms/step - loss: 0.2581\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2554\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 21s 98ms/step - loss: 0.2530 - val_loss: 0.2541\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2505\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2486\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 20s 97ms/step - loss: 0.2458 - val_loss: 0.2535\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 18s 86ms/step - loss: 0.2453\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 19s 88ms/step - loss: 0.2426\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 21s 99ms/step - loss: 0.2400 - val_loss: 0.2493\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 19s 88ms/step - loss: 0.2389\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 18s 87ms/step - loss: 0.2364\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2349 - val_loss: 0.2468\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2333\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2319\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 21s 100ms/step - loss: 0.2313 - val_loss: 0.2476\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 19s 91ms/step - loss: 0.2295\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 19s 88ms/step - loss: 0.2276\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 20s 97ms/step - loss: 0.2261 - val_loss: 0.2422\n",
      "Epochs: 10, Batch size: 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 10s 93ms/step - loss: 0.2212\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 10s 96ms/step - loss: 0.2181\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 11s 102ms/step - loss: 0.2182 - val_loss: 0.2422\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 10s 94ms/step - loss: 0.2163\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 10s 97ms/step - loss: 0.2155\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 11s 101ms/step - loss: 0.2138 - val_loss: 0.2418\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 10s 94ms/step - loss: 0.2138\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 10s 92ms/step - loss: 0.2133\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 11s 102ms/step - loss: 0.2112 - val_loss: 0.2423\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 10s 93ms/step - loss: 0.2114\n",
      "Epochs: 3, Batch size: 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 5s 99ms/step - loss: 0.2085\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 5s 98ms/step - loss: 0.2057\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 6s 111ms/step - loss: 0.2059 - val_loss: 0.2399\n",
      "Epochs: 3, Batch size: 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 3s 118ms/step - loss: 0.2033\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 3s 122ms/step - loss: 0.2023\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 0.2025 - val_loss: 0.2395\n",
      "Epochs: 5, Batch size: 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 69s 4s/step - loss: 0.2020\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 63s 4s/step - loss: 0.2013\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 71s 5s/step - loss: 0.2008 - val_loss: 0.2392\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 62s 4s/step - loss: 0.2008\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 61s 4s/step - loss: 0.1998\n",
      "Epochs: 5, Batch size: 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 54s 8s/step - loss: 0.2004\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 52s 7s/step - loss: 0.1993\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 58s 8s/step - loss: 0.1989 - val_loss: 0.2403\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 53s 7s/step - loss: 0.1993\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 54s 8s/step - loss: 0.1989\n",
      "Fold 3: MCRMSE 0.23991696423173128\n",
      "Fold 4 Start\n",
      "Epochs: 30, Batch size: 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 34s 88ms/step - loss: 0.4391\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.3499\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 23s 110ms/step - loss: 0.3311 - val_loss: 0.2942\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.3144\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 18s 87ms/step - loss: 0.3031\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2937 - val_loss: 0.2652\n",
      "Epoch 7/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2857\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 20s 94ms/step - loss: 0.2822\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 22s 104ms/step - loss: 0.2770 - val_loss: 0.2705\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2730\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 18s 86ms/step - loss: 0.2693\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2652 - val_loss: 0.2541\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 18s 86ms/step - loss: 0.2637\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 18s 86ms/step - loss: 0.2587\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2582 - val_loss: 0.2581\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 18s 88ms/step - loss: 0.2553\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 18s 88ms/step - loss: 0.2520\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2501 - val_loss: 0.2435\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2490\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 19s 89ms/step - loss: 0.2459\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2432 - val_loss: 0.2430\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 18s 87ms/step - loss: 0.2406\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 18s 88ms/step - loss: 0.2386\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2370 - val_loss: 0.2405\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 18s 88ms/step - loss: 0.2349\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 19s 90ms/step - loss: 0.2349\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 20s 95ms/step - loss: 0.2324 - val_loss: 0.2392\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 18s 87ms/step - loss: 0.2309\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 18s 86ms/step - loss: 0.2280\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 20s 97ms/step - loss: 0.2275 - val_loss: 0.2379\n",
      "Epochs: 10, Batch size: 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 9s 89ms/step - loss: 0.2218\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 10s 91ms/step - loss: 0.2201\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 10s 99ms/step - loss: 0.2185 - val_loss: 0.2346\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 10s 91ms/step - loss: 0.2186\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 10s 91ms/step - loss: 0.2171\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 11s 100ms/step - loss: 0.2162 - val_loss: 0.2350\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 9s 89ms/step - loss: 0.2155\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 9s 89ms/step - loss: 0.2151\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 10s 96ms/step - loss: 0.2145 - val_loss: 0.2361\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 10s 92ms/step - loss: 0.2133\n",
      "Epochs: 3, Batch size: 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 5s 96ms/step - loss: 0.2100\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 5s 94ms/step - loss: 0.2088\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 5s 104ms/step - loss: 0.2074 - val_loss: 0.2341\n",
      "Epochs: 3, Batch size: 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 3s 118ms/step - loss: 0.2066\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 3s 115ms/step - loss: 0.2056\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 3s 129ms/step - loss: 0.2051 - val_loss: 0.2334\n",
      "Epochs: 5, Batch size: 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 88s 6s/step - loss: 0.2043\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 85s 6s/step - loss: 0.2038\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 92s 7s/step - loss: 0.2034 - val_loss: 0.2338\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.2038\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.2023\n",
      "Epochs: 5, Batch size: 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 74s 10s/step - loss: 0.2033\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 73s 10s/step - loss: 0.2026\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 79s 11s/step - loss: 0.2020 - val_loss: 0.2335\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 74s 10s/step - loss: 0.2021\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 73s 10s/step - loss: 0.2018\n",
      "Fold 4: MCRMSE 0.234125416091627\n",
      "\n",
      "Cross-Validation Results\n",
      "Individual Fold MCRMSE Scores: [0.2323019532039205, 0.22499282996635284, 0.23751222515105547, 0.23991696423173128, 0.234125416091627]\n",
      "Mean MCRMSE across all folds: 0.23376987772893743\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "# Initialize prediction arrays with appropriate shapes\n",
    "# For train data predictions (oof - Out Of Fold predictions)\n",
    "preds_oof = np.zeros([len(X_node), X_node.shape[1], len(targets)])\n",
    "# For public test data predictions\n",
    "p_pub_total = np.zeros([len(X_node_pub), X_node_pub.shape[1], len(targets)])\n",
    "# For private test data predictions\n",
    "p_pri_total = np.zeros([len(X_node_pri), X_node_pri.shape[1], len(targets)])\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):\n",
    "    print(f\"Fold {fold} Start\")\n",
    "    \n",
    "    # Split data for current fold\n",
    "    X_node_tr, X_node_va = X_node[tr_idx], X_node[va_idx]\n",
    "    As_tr, As_va = As[tr_idx], As[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "    \n",
    "    # Initialize base model for current fold\n",
    "    base_model_fold = get_base(X_node.shape[2], As.shape[3])\n",
    "\n",
    "    # Load pre-trained weights\n",
    "    base_model_fold.load_weights(\"./base_ae.weights.h5\")\n",
    "        \n",
    "    # Get the full regression model\n",
    "    model = get_model(base_model_fold, seq_len_target_example, As.shape[3])\n",
    "\n",
    "    # Train the regression model with progressive batch sizes and epochs\n",
    "    for epochs, batch_size in zip([30, 10, 3, 3, 5, 5], [8, 16, 32, 64, 128, 256]):\n",
    "        print(f\"Epochs: {epochs}, Batch size: {batch_size}\")\n",
    "        if batch_size >= 128:\n",
    "            # Use CPU to avoid GPU memory issues\n",
    "            with tf.device('/CPU:0'):\n",
    "                history = model.fit([X_node_tr, As_tr], [y_tr],\n",
    "                                    validation_data=([X_node_va, As_va], [y_va]),\n",
    "                                    epochs=epochs,\n",
    "                                    batch_size=batch_size,\n",
    "                                    validation_freq=3,\n",
    "                                    verbose=1)\n",
    "        else:\n",
    "            history = model.fit([X_node_tr, As_tr], [y_tr],\n",
    "                                validation_data=([X_node_va, As_va], [y_va]),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                validation_freq=3,\n",
    "                                verbose=1)\n",
    "        \n",
    "    # Save model weights after training for the current fold\n",
    "    model.save_weights(f\"./model{fold}.weights.h5\")\n",
    "    \n",
    "    # Make out-of-fold predictions\n",
    "    p_fold_val = model.predict([X_node_va, As_va])\n",
    "    fold_mcrmse = mcrmse(y_va, p_fold_val, seq_len_target_example)\n",
    "    scores.append(fold_mcrmse)\n",
    "    print(f\"Fold {fold}: MCRMSE {scores[-1]}\")\n",
    "    \n",
    "    preds_oof[va_idx] = p_fold_val\n",
    "    \n",
    "    # Predict on public and private test sets for ensembling\n",
    "    p_pub_total += model.predict([X_node_pub, As_pub])\n",
    "    p_pri_total += model.predict([X_node_pri, As_pri])\n",
    "    \n",
    "    del X_node_tr, X_node_va, As_tr, As_va, y_tr, y_va, base_model_fold, model\n",
    "    gc.collect()\n",
    "\n",
    "# Average predictions from all folds\n",
    "p_pub_total /= kfold.n_splits\n",
    "p_pri_total /= kfold.n_splits\n",
    "\n",
    "# Save Out-Of-Fold predictions\n",
    "pd.to_pickle(preds_oof, \"oof.pkl\")\n",
    "\n",
    "print(\"\\nCross-Validation Results\")\n",
    "print(\"Individual Fold MCRMSE Scores:\", scores)\n",
    "print(\"Mean MCRMSE across all folds:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-19T12:02:36.523666Z",
     "iopub.status.idle": "2025-06-19T12:02:36.523960Z",
     "shell.execute_reply": "2025-06-19T12:02:36.523844Z",
     "shell.execute_reply.started": "2025-06-19T12:02:36.523832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reactivity</th>\n",
       "      <th>deg_Mg_pH10</th>\n",
       "      <th>deg_pH10</th>\n",
       "      <th>deg_Mg_50C</th>\n",
       "      <th>deg_50C</th>\n",
       "      <th>id_seqpos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791276</td>\n",
       "      <td>0.666923</td>\n",
       "      <td>1.715345</td>\n",
       "      <td>0.549008</td>\n",
       "      <td>0.724414</td>\n",
       "      <td>id_00073f8be_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.432134</td>\n",
       "      <td>3.419400</td>\n",
       "      <td>4.453869</td>\n",
       "      <td>3.341697</td>\n",
       "      <td>2.800921</td>\n",
       "      <td>id_00073f8be_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.735907</td>\n",
       "      <td>0.774205</td>\n",
       "      <td>0.850415</td>\n",
       "      <td>0.850344</td>\n",
       "      <td>0.801502</td>\n",
       "      <td>id_00073f8be_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.351261</td>\n",
       "      <td>1.151601</td>\n",
       "      <td>1.282416</td>\n",
       "      <td>1.576227</td>\n",
       "      <td>1.572157</td>\n",
       "      <td>id_00073f8be_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.841960</td>\n",
       "      <td>0.641961</td>\n",
       "      <td>0.657673</td>\n",
       "      <td>0.859995</td>\n",
       "      <td>0.873872</td>\n",
       "      <td>id_00073f8be_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C   deg_50C       id_seqpos\n",
       "0    0.791276     0.666923  1.715345    0.549008  0.724414  id_00073f8be_0\n",
       "1    2.432134     3.419400  4.453869    3.341697  2.800921  id_00073f8be_1\n",
       "2    1.735907     0.774205  0.850415    0.850344  0.801502  id_00073f8be_2\n",
       "3    1.351261     1.151601  1.282416    1.576227  1.572157  id_00073f8be_3\n",
       "4    0.841960     0.641961  0.657673    0.859995  0.873872  id_00073f8be_4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ls = []\n",
    "# Process public test set predictions\n",
    "for i, uid in enumerate(test_pub.id):\n",
    "    single_pred = p_pub_total[i]\n",
    "    single_df = pd.DataFrame(single_pred[:, :test_pub[\"seq_scored\"].iloc[i]], columns=targets)\n",
    "    single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "    preds_ls.append(single_df)\n",
    "\n",
    "# Process private test set predictions\n",
    "for i, uid in enumerate(test_pri.id):\n",
    "    single_pred = p_pri_total[i]\n",
    "    single_df = pd.DataFrame(single_pred[:, :test_pri[\"seq_scored\"].iloc[i]], columns=targets)\n",
    "    single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "    preds_ls.append(single_df)\n",
    "\n",
    "# Concatenate all predictions and save to submission.csv\n",
    "preds_df = pd.concat(preds_ls)\n",
    "preds_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "preds_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1600624,
     "sourceId": 22111,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
