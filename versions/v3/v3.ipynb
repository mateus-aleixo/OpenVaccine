{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:25.407171Z",
     "iopub.status.busy": "2025-06-19T11:48:25.406680Z",
     "iopub.status.idle": "2025-06-19T11:48:39.397776Z",
     "shell.execute_reply": "2025-06-19T11:48:39.396953Z",
     "shell.execute_reply.started": "2025-06-19T11:48:25.407147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Ensure the weights directory exists\n",
    "if not os.path.exists(\"./weights\"):\n",
    "    os.makedirs(\"./weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:39.399546Z",
     "iopub.status.busy": "2025-06-19T11:48:39.399046Z",
     "iopub.status.idle": "2025-06-19T11:48:39.404724Z",
     "shell.execute_reply": "2025-06-19T11:48:39.403942Z",
     "shell.execute_reply.started": "2025-06-19T11:48:39.399515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mcrmse(t, p, seq_len_target):\n",
    "    t = t[:, :seq_len_target]\n",
    "    p = p[:, :seq_len_target]\n",
    "    score = np.mean(np.sqrt(np.mean(np.mean((p - t) ** 2, axis=1), axis=0)))\n",
    "    return score\n",
    "\n",
    "\n",
    "def mcrmse_loss(t, y, seq_len_target):\n",
    "    t = t[:, :seq_len_target]\n",
    "    y = y[:, :seq_len_target]\n",
    "    loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean(tf.reduce_mean((t - y) ** 2, axis=1), axis=0)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Layer for the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:39.405789Z",
     "iopub.status.busy": "2025-06-19T11:48:39.405550Z",
     "iopub.status.idle": "2025-06-19T11:48:39.439211Z",
     "shell.execute_reply": "2025-06-19T11:48:39.438530Z",
     "shell.execute_reply.started": "2025-06-19T11:48:39.405772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AutoencoderLossLayer(L.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AutoencoderLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        node, p = inputs\n",
    "        # Custom loss for denoising autoencoder (binary cross-entropy like)\n",
    "        loss = -tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention and Model Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:39.440901Z",
     "iopub.status.busy": "2025-06-19T11:48:39.440690Z",
     "iopub.status.idle": "2025-06-19T11:48:39.460521Z",
     "shell.execute_reply": "2025-06-19T11:48:39.459824Z",
     "shell.execute_reply.started": "2025-06-19T11:48:39.440885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def attention(x_inner, x_outer, n_factor):\n",
    "    x_Q = L.Conv1D(n_factor, 1, activation='linear',\n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   bias_initializer='glorot_uniform')(x_inner)\n",
    "    x_K = L.Conv1D(n_factor, 1, activation='linear',\n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   bias_initializer='glorot_uniform')(x_outer)\n",
    "    x_V = L.Conv1D(n_factor, 1, activation='linear',\n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   bias_initializer='glorot_uniform')(x_outer)\n",
    "    x_KT = L.Permute((2, 1))(x_K)\n",
    "    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n",
    "    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n",
    "    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n",
    "    return att\n",
    "\n",
    "\n",
    "def multi_head_attention(x, y, n_factor, n_head, dropout):\n",
    "    if n_head == 1:\n",
    "        att = attention(x, y, n_factor)\n",
    "    else:\n",
    "        n_factor_head = n_factor // n_head\n",
    "        heads = [attention(x, y, n_factor_head) for _ in range(n_head)]\n",
    "        att = L.Concatenate()(heads)\n",
    "        att = L.Dense(n_factor,\n",
    "                      kernel_initializer='glorot_uniform',\n",
    "                      bias_initializer='glorot_uniform')(att)\n",
    "    x = L.Add()([x, att])\n",
    "    x = L.LayerNormalization()(x)\n",
    "    if dropout > 0:\n",
    "        x = L.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def res(x, unit, kernel=3, rate=0.2):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding=\"same\", activation=None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    return L.Add()([x, h])\n",
    "\n",
    "\n",
    "def forward(x, unit, kernel=3, rate=0.2):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding=\"same\", activation=None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = res(h, unit, kernel, rate)\n",
    "    return h\n",
    "\n",
    "\n",
    "def adj_attn(x, adj, unit, n=2, rate=0.2):\n",
    "    x_a = x\n",
    "    x_as = []\n",
    "    for _ in range(n):\n",
    "        x_a = forward(x_a, unit)\n",
    "        x_a = L.Lambda(lambda inputs: tf.matmul(inputs[0], inputs[1]))([adj, x_a])\n",
    "        x_as.append(x_a)\n",
    "    if n == 1:\n",
    "        x_a = x_as[0]\n",
    "    else:\n",
    "        x_a = L.Concatenate()(x_as)\n",
    "    x_a = forward(x_a, unit)\n",
    "    return x_a\n",
    "\n",
    "\n",
    "def get_base(X_node_shape_2, As_shape_3):\n",
    "    node = tf.keras.Input(shape=(None, X_node_shape_2), name=\"node\")\n",
    "    adj = tf.keras.Input(shape=(None, None, As_shape_3), name=\"adj\")\n",
    "\n",
    "    # Learnable adjacency component\n",
    "    adj_learned = L.Dense(1, \"relu\")(adj)\n",
    "    adj_all = L.Concatenate(axis=3)([adj, adj_learned])\n",
    "\n",
    "    # Initial feature extraction using multiple Conv1D layers with different kernel sizes\n",
    "    xs = []\n",
    "    xs.append(node)\n",
    "    x1 = forward(node, 192, kernel=3, rate=0.2)\n",
    "    x2 = forward(x1, 128, kernel=6, rate=0.2)\n",
    "    x3 = forward(x2, 96, kernel=15, rate=0.2)\n",
    "    x4 = forward(x3, 64, kernel=30, rate=0.2)\n",
    "    x = L.Concatenate()([x1, x2, x3, x4])\n",
    "\n",
    "    # Graph convolution and multi-head attention layers\n",
    "    for unit in [128, 96, 64]:\n",
    "        x_as = []\n",
    "        for i in range(adj_all.shape[3]):\n",
    "            adj_slice = L.Lambda(lambda inputs, idx=i: inputs[:, :, :, idx])(adj_all)\n",
    "            x_a = adj_attn(x, adj_slice, unit, rate=0.2)\n",
    "            x_as.append(x_a)\n",
    "        x_c = forward(x, unit, kernel=30, rate=0.2)\n",
    "\n",
    "        x = L.Concatenate()(x_as + [x_c])\n",
    "        x = forward(x, unit, rate=0.2)\n",
    "        x = multi_head_attention(x, x, unit, 8, 0.2)\n",
    "        xs.append(x)\n",
    "\n",
    "    x = L.Concatenate()(xs)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node, adj], outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ae_model(base_model, X_node_shape_2):\n",
    "    node = tf.keras.Input(shape=(None, X_node_shape_2), name=\"node\")\n",
    "    adj = tf.keras.Input(shape=(None, None, base_model.input[1].shape[3]), name=\"adj\")\n",
    "\n",
    "    # Apply spatial dropout to the node input for denoising\n",
    "    x = base_model([L.SpatialDropout1D(0.5)(node), adj])    \n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    x = L.Dropout(0.4)(x)\n",
    "    p = L.Dense(X_node_shape_2, activation='sigmoid')(x)\n",
    "\n",
    "    loss = AutoencoderLossLayer()([node, p])\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node, adj], outputs=[loss])\n",
    "\n",
    "    opt = tf.optimizers.Adam()\n",
    "    model.compile(optimizer=opt, loss=lambda _, y: y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(base_model, seq_len_target, As_shape_3):\n",
    "    node = tf.keras.Input(shape=(None, base_model.input[0].shape[2]), name=\"node\")\n",
    "    adj = tf.keras.Input(shape=(None, None, As_shape_3), name=\"adj\")\n",
    "\n",
    "    x = base_model([node, adj])\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.6)(x)\n",
    "    x = L.Dense(5, activation=None)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node, adj], outputs=[x])\n",
    "\n",
    "    opt = tf.optimizers.Adam()\n",
    "    # Using the custom MCRMSE loss for compilation\n",
    "    model.compile(optimizer=opt, loss=lambda t, y: mcrmse_loss(t, y, seq_len_target))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:48:39.461536Z",
     "iopub.status.busy": "2025-06-19T11:48:39.461283Z",
     "iopub.status.idle": "2025-06-19T11:49:19.950143Z",
     "shell.execute_reply": "2025-06-19T11:49:19.949542Z",
     "shell.execute_reply.started": "2025-06-19T11:48:39.461519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_json(\"../../stanford-covid-vaccine/train.json\", lines=True)\n",
    "test  = pd.read_json(\"../../stanford-covid-vaccine/test.json\", lines=True)\n",
    "sub = pd.read_csv(\"../../stanford-covid-vaccine/sample_submission.csv\")\n",
    "\n",
    "# # Filter training data\n",
    "train = train[train.signal_to_noise > 1].reset_index(drop=True)\n",
    "\n",
    "# Separate public and private test data based on sequence length\n",
    "test_pub = test[test[\"seq_length\"] == 107].reset_index(drop=True)\n",
    "test_pri = test[test[\"seq_length\"] == 130].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Load BPP (Base Pair Probability) matrices\n",
    "def load_bpps(df, path=\"../../stanford-covid-vaccine/bpps/\"):\n",
    "    \"\"\"Loads BPP matrices for a given DataFrame.\"\"\"\n",
    "    bpps = []\n",
    "    for id_val in tqdm(df[\"id\"], desc=\"Loading BPPs\"):\n",
    "        bpps.append(np.load(f\"{path}{id_val}.npy\"))\n",
    "    return np.array(bpps)\n",
    "\n",
    "\n",
    "As_bpp_train = load_bpps(train)\n",
    "As_bpp_pub = load_bpps(test_pub)\n",
    "As_bpp_pri = load_bpps(test_pri)\n",
    "\n",
    "# Get target column names from sample submission\n",
    "targets = list(sub.columns[1:])\n",
    "\n",
    "# Prepare y_train (target values for training)\n",
    "seq_len_train_example = train[\"seq_length\"].iloc[0]\n",
    "seq_len_target_example = train[\"seq_scored\"].iloc[0]\n",
    "ignore_value = -10000\n",
    "ignore_length = seq_len_train_example - seq_len_target_example\n",
    "\n",
    "y_train = []\n",
    "for target in targets:\n",
    "    y = np.vstack(train[target])\n",
    "    # Pad with ignore_value for positions not scored\n",
    "    dummy = np.zeros([y.shape[0], ignore_length]) + ignore_value\n",
    "    y = np.hstack([y, dummy])\n",
    "    y_train.append(y)\n",
    "y = np.stack(y_train, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:49:19.951136Z",
     "iopub.status.busy": "2025-06-19T11:49:19.950891Z",
     "iopub.status.idle": "2025-06-19T11:49:28.724989Z",
     "shell.execute_reply": "2025-06-19T11:49:28.724214Z",
     "shell.execute_reply.started": "2025-06-19T11:49:19.951118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_structure_adj(df):\n",
    "    Ss = []\n",
    "    for i in tqdm(range(len(df)), desc=\"Getting Structure Adjacency\"):\n",
    "        seq_length = df[\"seq_length\"].iloc[i]\n",
    "        structure = df[\"structure\"].iloc[i]\n",
    "        sequence = df[\"sequence\"].iloc[i]\n",
    "\n",
    "        cue = []\n",
    "        # Separate adjacency matrices for different base pairs, then sum them up\n",
    "        a_structures = {\n",
    "            (\"A\", \"U\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"C\", \"G\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"G\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"A\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"C\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"U\"): np.zeros([seq_length, seq_length]),\n",
    "        }\n",
    "        \n",
    "        for j in range(seq_length):\n",
    "            if structure[j] == \"(\":\n",
    "                cue.append(j)\n",
    "            elif structure[j] == \")\":\n",
    "                if cue:\n",
    "                    start = cue.pop()\n",
    "                    a_structures[(sequence[start], sequence[j])][start, j] = 1\n",
    "                    a_structures[(sequence[j], sequence[start])][j, start] = 1\n",
    "        \n",
    "        # Sum all specific base pair adjacency matrices into a single one\n",
    "        a_strc = np.stack([a for a in a_structures.values()], axis=2)\n",
    "        a_strc = np.sum(a_strc, axis=2, keepdims=True)\n",
    "        Ss.append(a_strc)\n",
    "    \n",
    "    return np.array(Ss)\n",
    "\n",
    "\n",
    "Ss = get_structure_adj(train)\n",
    "Ss_pub = get_structure_adj(test_pub)\n",
    "Ss_pri = get_structure_adj(test_pri)\n",
    "\n",
    "\n",
    "def get_distance_matrix(bpps_array):\n",
    "    seq_length = bpps_array.shape[1]\n",
    "    idx = np.arange(seq_length)\n",
    "    Ds = []\n",
    "    for i in range(len(idx)):\n",
    "        d = np.abs(idx[i] - idx)\n",
    "        Ds.append(d)\n",
    "\n",
    "    Ds = np.array(Ds) + 1\n",
    "    Ds = 1 / Ds\n",
    "    Ds = Ds[None, :, :]\n",
    "    Ds = np.repeat(Ds, len(bpps_array), axis=0)\n",
    "\n",
    "    Dss = []\n",
    "    for i in [1, 2, 4]:\n",
    "        Dss.append(Ds ** i)\n",
    "    Ds = np.stack(Dss, axis=3)\n",
    "    return Ds\n",
    "\n",
    "\n",
    "Ds = get_distance_matrix(As_bpp_train)\n",
    "Ds_pub = get_distance_matrix(As_bpp_pub)\n",
    "Ds_pri = get_distance_matrix(As_bpp_pri)\n",
    "\n",
    "# Concatenate all adjacency features: BPPs, Structure Adjacency, Distance Adjacency\n",
    "As = np.concatenate([As_bpp_train[:, :, :, None], Ss, Ds], axis=3).astype(np.float32)\n",
    "As_pub = np.concatenate([As_bpp_pub[:, :, :, None], Ss_pub, Ds_pub], axis=3).astype(\n",
    "    np.float32\n",
    ")\n",
    "As_pri = np.concatenate([As_bpp_pri[:, :, :, None], Ss_pri, Ds_pri], axis=3).astype(\n",
    "    np.float32\n",
    ")\n",
    "\n",
    "# Free up memory\n",
    "del Ss, Ds, Ss_pub, Ds_pub, Ss_pri, Ds_pri\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:49:28.726159Z",
     "iopub.status.busy": "2025-06-19T11:49:28.725968Z",
     "iopub.status.idle": "2025-06-19T11:49:29.874157Z",
     "shell.execute_reply": "2025-06-19T11:49:29.873603Z",
     "shell.execute_reply.started": "2025-06-19T11:49:28.726144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def return_ohe(n, i):\n",
    "    tmp = [0] * n\n",
    "    tmp[i] = 1\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def get_base_features_and_a(df, bpps_array):\n",
    "    # One-hot encode sequence (bases)\n",
    "    mapping_node = {s: return_ohe(4, i) for i, s in enumerate([\"A\", \"G\", \"C\", \"U\"])}\n",
    "    X_node_seq = np.stack(\n",
    "        df[\"sequence\"].apply(lambda x: list(map(lambda y: mapping_node[y], list(x))))\n",
    "    )\n",
    "\n",
    "    # One-hot encode predicted loop type\n",
    "    mapping_loop = {\n",
    "        s: return_ohe(7, i) for i, s in enumerate([\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"])\n",
    "    }\n",
    "    X_loop = np.stack(\n",
    "        df[\"predicted_loop_type\"].apply(\n",
    "            lambda x: list(map(lambda y: mapping_loop[y], list(x)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Count of paired bases with high probability (BPP > 0.9)\n",
    "    bpp_paired_count = np.sum(bpps_array > 0.9, axis=2)[:, :, None]\n",
    "\n",
    "    # Concatenate sequence, loop, and the new BPP-derived feature\n",
    "    X_node_base = np.concatenate([X_node_seq, X_loop, bpp_paired_count], axis=2)\n",
    "\n",
    "    # Calculate 'a' which represents unique combinations before OHE\n",
    "    # This 'a' will be used to derive a global vocabulary.\n",
    "    a = np.sum(\n",
    "        X_node_base * (2 ** np.arange(X_node_base.shape[2])[None, None, :]), axis=2\n",
    "    )\n",
    "    return X_node_base, a\n",
    "\n",
    "\n",
    "def apply_interaction_ohe(X_node_base, a_values, global_vocab):\n",
    "    ohes = []\n",
    "    # Create one-hot vectors based on the global vocabulary\n",
    "    for v in global_vocab:\n",
    "        ohes.append(a_values == v)\n",
    "    ohes = np.stack(ohes, axis=2)\n",
    "\n",
    "    return np.concatenate([X_node_base, ohes], axis=2).astype(np.float32)\n",
    "\n",
    "\n",
    "# Get base features and intermediate 'a' for all datasets\n",
    "# This is done first to get all 'a' values before determining the global_vocab\n",
    "X_node_base_train, a_train = get_base_features_and_a(train, As_bpp_train)\n",
    "X_node_base_pub, a_pub = get_base_features_and_a(test_pub, As_bpp_pub)\n",
    "X_node_base_pri, a_pri = get_base_features_and_a(test_pri, As_bpp_pri)\n",
    "\n",
    "# Combine all 'a' values from train, public test, and private test\n",
    "# to get a comprehensive, global vocabulary for interaction features.\n",
    "a_all_combined = np.concatenate([a_train.flatten(), a_pub.flatten(), a_pri.flatten()])\n",
    "global_vocab = sorted(set(a_all_combined))\n",
    "\n",
    "# Now apply the consistent OHE for interaction features using the global_vocab\n",
    "X_node = apply_interaction_ohe(X_node_base_train, a_train, global_vocab)\n",
    "X_node_pub = apply_interaction_ohe(X_node_base_pub, a_pub, global_vocab)\n",
    "X_node_pri = apply_interaction_ohe(X_node_base_pri, a_pri, global_vocab)\n",
    "\n",
    "# Free up memory from raw bpp arrays\n",
    "del As_bpp_train, As_bpp_pub, As_bpp_pri\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training the Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:49:29.875104Z",
     "iopub.status.busy": "2025-06-19T11:49:29.874843Z",
     "iopub.status.idle": "2025-06-19T12:02:33.576146Z",
     "shell.execute_reply": "2025-06-19T12:02:33.575579Z",
     "shell.execute_reply.started": "2025-06-19T11:49:29.875078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dynamically get input shapes for model construction\n",
    "base_model_ae = get_base(X_node.shape[2], As.shape[3])\n",
    "ae_model = get_ae_model(base_model_ae, X_node.shape[2])\n",
    "\n",
    "# Train denoising autoencoder using all data (train, public test, private test)\n",
    "dummy_ae_target = np.zeros_like(X_node[:, :, 0])\n",
    "dummy_ae_target_pub = np.zeros_like(X_node_pub[:, :, 0])\n",
    "dummy_ae_target_pri = np.zeros_like(X_node_pri[:, :, 0])\n",
    "\n",
    "for i in range(60 // 5):\n",
    "    print(f\"Pre-training Iteration {i+1}/{60 // 5}\")\n",
    "    print(\"Training on main dataset\")\n",
    "    ae_model.fit([X_node, As], [dummy_ae_target],\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "    print(\"Training on public test dataset\")\n",
    "    ae_model.fit([X_node_pub, As_pub], [dummy_ae_target_pub],\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "    print(\"Training on private test dataset\")\n",
    "    ae_model.fit([X_node_pri, As_pri], [dummy_ae_target_pri],\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "    gc.collect()\n",
    "base_model_ae.save_weights(\"./weights/base_ae.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training (Regression) with KFold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T12:03:27.529365Z",
     "iopub.status.busy": "2025-06-19T12:03:27.529091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "# Initialize prediction arrays with appropriate shapes\n",
    "# For train data predictions (oof - Out Of Fold predictions)\n",
    "preds_oof = np.zeros([len(X_node), X_node.shape[1], len(targets)])\n",
    "# For public test data predictions\n",
    "p_pub_total = np.zeros([len(X_node_pub), X_node_pub.shape[1], len(targets)])\n",
    "# For private test data predictions\n",
    "p_pri_total = np.zeros([len(X_node_pri), X_node_pri.shape[1], len(targets)])\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):\n",
    "    print(f\"Fold {fold} Start\")\n",
    "\n",
    "    # Split data for current fold\n",
    "    X_node_tr, X_node_va = X_node[tr_idx], X_node[va_idx]\n",
    "    As_tr, As_va = As[tr_idx], As[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    # Initialize base model for current fold\n",
    "    base_model_fold = get_base(X_node.shape[2], As.shape[3])\n",
    "\n",
    "    # Load pre-trained weights\n",
    "    base_model_fold.load_weights(\"./weights/base_ae.weights.h5\")\n",
    "\n",
    "    # Get the full regression model\n",
    "    model = get_model(base_model_fold, seq_len_target_example, As.shape[3])\n",
    "\n",
    "    # Callbacks for main training\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=5, min_lr=0.000005, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    print(\"Epochs: 100, Batch size: 64\")\n",
    "    history = model.fit(\n",
    "        [X_node_tr, As_tr],\n",
    "        [y_tr],\n",
    "        validation_data=([X_node_va, As_va], [y_va]),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Save model weights after training for the current fold\n",
    "    model.save_weights(f\"./weights/model{fold}.weights.h5\")\n",
    "\n",
    "    # Make out-of-fold predictions\n",
    "    p_fold_val = model.predict([X_node_va, As_va])\n",
    "    fold_mcrmse = mcrmse(y_va, p_fold_val, seq_len_target_example)\n",
    "    scores.append(fold_mcrmse)\n",
    "    print(f\"Fold {fold}: MCRMSE {scores[-1]}\")\n",
    "\n",
    "    preds_oof[va_idx] = p_fold_val\n",
    "\n",
    "    # Predict on public and private test sets for ensembling\n",
    "    p_pub_total += model.predict([X_node_pub, As_pub])\n",
    "    p_pri_total += model.predict([X_node_pri, As_pri])\n",
    "\n",
    "    del X_node_tr, X_node_va, As_tr, As_va, y_tr, y_va, base_model_fold, model\n",
    "    gc.collect()\n",
    "\n",
    "# Average predictions from all folds\n",
    "p_pub_total /= kfold.n_splits\n",
    "p_pri_total /= kfold.n_splits\n",
    "\n",
    "# Save Out-Of-Fold predictions\n",
    "pd.to_pickle(preds_oof, \"oof.pkl\")\n",
    "\n",
    "print(\"\\nCross-Validation Results\")\n",
    "print(\"Individual Fold MCRMSE Scores:\", scores)\n",
    "print(\"Mean MCRMSE across all folds:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-19T12:02:36.523666Z",
     "iopub.status.idle": "2025-06-19T12:02:36.523960Z",
     "shell.execute_reply": "2025-06-19T12:02:36.523844Z",
     "shell.execute_reply.started": "2025-06-19T12:02:36.523832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds_ls = []\n",
    "# Process public test set predictions\n",
    "for i, uid in enumerate(test_pub.id):\n",
    "    single_pred = p_pub_total[i]\n",
    "    single_df = pd.DataFrame(single_pred[:, :test_pub[\"seq_scored\"].iloc[i]], columns=targets)\n",
    "    single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "    preds_ls.append(single_df)\n",
    "\n",
    "# Process private test set predictions\n",
    "for i, uid in enumerate(test_pri.id):\n",
    "    single_pred = p_pri_total[i]\n",
    "    single_df = pd.DataFrame(single_pred[:, :test_pri[\"seq_scored\"].iloc[i]], columns=targets)\n",
    "    single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "    preds_ls.append(single_df)\n",
    "\n",
    "# Concatenate all predictions and save to submission.csv\n",
    "preds_df = pd.concat(preds_ls)\n",
    "preds_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "preds_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1600624,
     "sourceId": 22111,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
